# -*- coding: utf-8 -*-
"""랜덤포레스트 (원본) 최종 - 주연.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U88LK7p9pzXorzguMeP7oB9qzMFDrpPI
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, average_precision_score, roc_auc_score
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier

#obtuna
# 각 모델 정확도를 제외한 모든 수치가 낮아 실사용 불가, optuna를 이용한 튜닝 수행
# !pip install --quiet optuna

import optuna
import itertools
import sklearn.svm
import sklearn.model_selection
from sklearn.metrics import classification_report

#평가 AUC
from sklearn.metrics import roc_auc_score

# 데이터 로드 및 전처리
pd.set_option('display.max_columns', None)
df = pd.read_csv('/content/drive/MyDrive/심화프로젝트 ML/train.csv')

nan_data = df.drop(columns=['id', 'CustomerId'])
data = nan_data[~nan_data.duplicated()]

le = LabelEncoder()
data['Gender'] = le.fit_transform(data["Gender"])

oe = OneHotEncoder()
geo_csr = oe.fit_transform(data[['Geography']])
geo_df = pd.DataFrame(geo_csr.toarray(), columns=oe.get_feature_names_out())

data = pd.concat([data.reset_index(drop=True), geo_df.reset_index(drop=True)], axis=1)
data.drop(columns=['Surname', 'Geography'], inplace=True)

X = data.drop(columns=['Exited']).reset_index(drop=True)
y = data['Exited'].reset_index(drop=True)

X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)

# SMOTE 적용
sm = SMOTE(random_state=42)
X_train_res, y_train_res = sm.fit_resample(X_train, y_train)

# Optuna를 이용한 하이퍼파라미터 튜닝
def objective(trial):
    params = {
        "n_estimators": trial.suggest_int("n_estimators", 100, 500),
        "max_depth": trial.suggest_int("max_depth", 3, 50),
        "min_samples_split": trial.suggest_int("min_samples_split", 2, 20),
        "min_samples_leaf": trial.suggest_int("min_samples_leaf", 1, 10),
        "max_features": trial.suggest_categorical("max_features", ["sqrt", "log2"]),
    }

    rf = RandomForestClassifier(**params, class_weight="balanced", random_state=42, n_jobs=-1)
    rf.fit(X_train_res, y_train_res)
    y_prob = rf.predict_proba(X_test)[:, 1]

    best_recall = 0
    best_threshold = 0.5

    for threshold in np.arange(0.1, 0.9, 0.01):
        y_pred = (y_prob >= threshold).astype(int)
        recall = recall_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred)

        # Precision ≥ 0.5 조건을 만족하면서 Recall 최대로 뽑기
        if precision >= 0.5 and recall > best_recall:
            best_recall = recall
            best_threshold = threshold
    trial.set_user_attr("best_threshold", best_threshold)

    return best_recall

# Optuna 실행
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=20, n_jobs=-1)

# 최적 threshold 값 가져오기
best_threshold = study.best_trial.user_attrs["best_threshold"]

# 최적 모델 학습
best_params = study.best_trial.params
best_model = RandomForestClassifier(**best_params, random_state=42, n_jobs=-1)
best_model.fit(X_train_res, y_train_res)

y_prob_test = best_model.predict_proba(X_test)[:, 1]

# 최종 평가
y_test_pred_final = (y_prob_test >= best_threshold).astype(int)

print("최적 하이퍼파라미터:", best_params)
print(f"최적 Threshold: {best_threshold:.2f}")
print("Accuracy:", accuracy_score(y_test, y_test_pred_final))
print("F1 Score:", f1_score(y_test, y_test_pred_final))
print("Recall:", recall_score(y_test, y_test_pred_final))
print("Precision:", precision_score(y_test, y_test_pred_final))
print("Average Precision:", average_precision_score(y_test, y_prob_test))
print("ROC AUC Score:", roc_auc_score(y_test, y_prob_test))

"""
최적 하이퍼파라미터: {'n_estimators': 213, 'max_depth': 29, 'min_samples_split': 2, 'min_samples_leaf': 9, 'max_features': 'sqrt'}
최적 Threshold: 0.24
Accuracy: 0.7884895138804704
F1 Score: 0.6222871678302755
Recall: 0.8231672394043528
Precision: 0.5002175237100844
Average Precision: 0.7149796276274035
ROC AUC Score: 0.8847185683002241
"""

# 랜덤 포레스트 기반 K-Means Clustering

clf_dt = RandomForestClassifier(random_state=42,
        max_depth=29,
        min_samples_split=2,
        min_samples_leaf=9,
        n_estimators=213,
        max_features='sqrt')

clf_dt.fit(X_train, y_train)

y_pred = clf_dt.predict(X_test)

for name, value in zip(X.columns.tolist(), clf_dt.feature_importances_):
    print('{0}: {1:.3f}'.format(name, value))

sns.barplot(x=clf_dt.feature_importances_, y=X.columns.tolist())

# Age, NumOfProducts, IsActiveMember 변수 기준으로 K-Means clustring 진행
from sklearn.cluster import KMeans

X_cluster = df[['Age', 'NumOfProducts', 'IsActiveMember']]

# 각 K값에 대한 오차제곱합 계산 및 시각화
SSE = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_cluster)
    SSE.append(kmeans.inertia_)

plt.plot(range(1,11), SSE, marker='o')
plt.xlabel('value of K')
plt.ylabel('inertia(SSE)')
plt.show()

diff = np.diff(SSE)
optimal_k = np.argmin(diff[1:] - diff[:1]) +2
print(optimal_k)

# 경사가 가장 완만해지는 K값 = 2이나 그룹 간 특징을 산출하기 위해 k=4 사용, 1,2번 그룹이 이탈률이 상대적으로 높음
kmeans = KMeans(n_clusters=4, random_state=42)
df['Cluster'] = kmeans.fit_predict(X_cluster)
df[['Age', 'NumOfProducts','IsActiveMember' ,'Cluster']].head()

cluster_analysis = df.groupby('Cluster')['Exited'].mean()
print(cluster_analysis)

cluster_summery = df.groupby('Cluster').mean(numeric_only=True).drop(['id', 'CustomerId'], axis=1)
cluster_summery

# 클러스터별 Age 분포
plt.figure(figsize=(8, 5))
sns.boxplot(x="Cluster", y="Age", data=df)
plt.title("Age Distribution by Cluster")
plt.show()

# 클러스터별 NumOfProducts 분포
plt.figure(figsize=(8, 5))
sns.boxplot(x="Cluster", y="NumOfProducts", data=df)
plt.title("NumOfProducts Distribution by Cluster")
plt.show()

# 클러스터별 IsActiveMember 분포
plt.figure(figsize=(8, 5))
sns.boxplot(x="Cluster", y="IsActiveMember", data=df)
plt.title("IsActiveMember Distribution by Cluster")
plt.show()

sns.pairplot(df, hue="Cluster", vars=["Age", "NumOfProducts", "IsActiveMember"])
plt.show()

"""
- 각 그룹별 특징 정리 or 페르소나 부여
    - 0. 평균 30대 중반의 사용 상품이 높으며 활동 비율이 약 절반인 그룹 : 연차가 어느정도 쌓인 직장인급 주거래 고객(이탈률 낮음)
    - 1. 평균 50대 중반(최고령자)의 사용 상품이 제일 낮으며 가장 많이 활동하는 그룹 : 은퇴 전후 노후계획을 바라보는 고령 고객(이탈률 높음)
    - 2. 평균 40대 초반의 사용 상품이 비교적 낮고 활동 비율이 가장 낮은 그룹 : 연차가 비교적 많이 쌓인 중견급 직장인(이탈률 높아질 가능성 있음)
    - 3. 평균 20대 중후반의 사용 상품이 높으며 활동 비율이 약 절반인 그룹 : 사회 초년생(이탈률 낮음)
"""

"""
- 각 그룹별 이탈 방지 or 이탈률 회복을 위한 전략 제시
    - 0, 3. 사용 상품 수를 유지하거나, 지속적으로 늘릴 수 있는 프로모션 진행(그룹별 나이에 맞는 프로모션 제공, 사회 초년생 전용 적금 or 직장인을 위한 저금리 대출 상품, 자취 및 외식, 문화 생활에 혜택이 있는 카드)
    - 1, 2. 중장년 + 고령층 인구를 타겟으로 하는 프로모션(증여, 미술 투자 자문 제공 + 시니어라운지 및 어르신 전용 프로그램 운영 + 일정 금액 초과 주택 역모기지 출시 + 장기투자를 위한 다양한 금융상품 정보 제공)
    - 
"""